{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a032d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import progressbar\n",
    "import codecs\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.attrs import ORTH\n",
    "import re\n",
    "import string\n",
    "import pathlib\n",
    "from tqdm import tqdm \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk.data\n",
    "\n",
    "from time import time\n",
    "import random\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cdb102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sentencizer(doc):\n",
    "    ''' Look for sentence start tokens by scanning for periods only. '''\n",
    "    split_lowercase = all(w.text.islower() for w in doc)\n",
    "    \n",
    "    for i, token in enumerate(doc[:-2]):  # The last token cannot start a sentence\n",
    "        if token.text[0] == \".\" or token.text[-1] == \".\":\n",
    "            if not split_lowercase and (not doc[i+1].text[0].isupper() or doc[i+2].text[0] == '.'):# or doc[i+1].text[0] == '.':\n",
    "                    doc[i+1].is_sent_start = False  # Tell the default sentencizer to ignore this token\n",
    "            # pass\n",
    "        else:\n",
    "            doc[i+1].is_sent_start = False  # Tell the default sentencizer to ignore this token\n",
    "    return doc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def custom_splitter(text = None):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(custom_sentencizer, before = \"parser\")\n",
    "    \n",
    "    special_cases = {\"Rs.\": \"rs.\", \"No.\": \"no.\", \"no.\": \"no.\", \"i.e.\": \"i.e.\", \"viz.\": \"viz.\", \"M/s.\": \"m/s.\", \"Mohd.\": \"mohd.\", \"Ex.\": \"exhibit\", \"Art.\" : \"article\", \"Arts.\" : \"articles\", \"S.\": \"section\", \"s.\": \"section\", \"ss.\": \"sections\", \"u/s.\": \"section\", \"u/ss.\": \"sections\", \"art.\": \"article\", \"arts.\": \"articles\", \"u/arts.\" : \"articles\", \"u/art.\" : \"article\", \"hon'ble\" : \"honourable\", \"ITO\" : \"Ito\", \"UBI\" : \"Ubi\", \"Ors.\" : \"ors.\"}    \n",
    "#     special_cases = {\"Rs.\": \"rs.\", \"No.\": \"no.\", \"no.\": \"no.\", \"v.\": \"vs\", \"vs.\": \"vs\", \"i.e.\": \"i.e.\", \"viz.\": \"viz.\", \"M/s.\": \"m/s.\", \"Mohd.\": \"mohd.\", \"Ex.\": \"exhibit\", \"Art.\" : \"article\", \"Arts.\" : \"articles\", \"S.\": \"section\", \"s.\": \"section\", \"ss.\": \"sections\", \"u/s.\": \"section\", \"u/ss.\": \"sections\", \"art.\": \"article\", \"arts.\": \"articles\", \"u/arts.\" : \"articles\", \"u/art.\" : \"article\", \"hon'ble\" : \"honourable\"}\n",
    "    #Ltd. Pvt. Corp.\n",
    "\n",
    "\n",
    "    for case, orth in special_cases.items():\n",
    "    \tnlp.tokenizer.add_special_case(case, [{ORTH: orth}])\n",
    "    \n",
    "    \n",
    "    if text is None: return nlp\n",
    "    #text = text.strip()\n",
    "    #print (text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    #text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parsed = nlp(text)\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for sent in parsed.sents:\n",
    "        sentences.append(sent.text)\n",
    "    \n",
    "    return sentences, nlp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class custom_tokenizer:\n",
    "        def __init__(self):\n",
    "                # self.NLP = spacy.load('en_core_web_sm')\n",
    "                self.NLP = custom_splitter()\n",
    "                puncts = string.punctuation.replace('.', '').replace('-', '')\n",
    "                self.trans = str.maketrans('.-','  ', puncts)\n",
    "                \n",
    "        def to_words(self, text):\n",
    "                text = re.sub('\\n', ' ', text.lower())\n",
    "                text = re.sub('\\s+', ' ', text).strip()\n",
    "                \n",
    "                words = [s.text.lower() if s.text[0] == \"'\" and len(s.text) == 2 else s.text.translate(self.trans).strip().lower() for s in self.NLP(text.strip()) if not s.is_punct]\n",
    "                \n",
    "                return words \n",
    "        \n",
    "        def to_sentences(self, text):\n",
    "                #remove extra dots\n",
    "                text = re.sub('\\.\\s*\\.\\s*\\.', '. ', text)\n",
    "                text = re.sub('\\.\\s*\\.', '. ', text)\n",
    "                \n",
    "                #remove dash\n",
    "                text = re.sub('-', ' ', text)\n",
    "                \n",
    "                # remove extra whitespace\n",
    "                text = re.sub('\\n', ' ', text)\n",
    "                text = re.sub('\\s+', ' ', text).strip()\n",
    "                \n",
    "                \n",
    "                \n",
    "                sentences = [s.text for s in self.NLP(text).sents if len(s.text.strip()) > 5]\n",
    "                # if re.match('\\d+\\.?.*', text):\n",
    "                #         text = text[4:]\n",
    "                \n",
    "                return sentences\n",
    "        \n",
    "        def to_cleaned_sents(self, text):\n",
    "                sents = self.to_sentences(text)\n",
    "                words = [' '.join(self.to_words(s)) + '.' for s in sents]\n",
    "                return words        \n",
    "        \n",
    "       \n",
    "        \n",
    "       \n",
    "        \n",
    "class simple_tokenizer:\n",
    "        def to_words(self, s):\n",
    "                s = s.strip().strip('.').strip()\n",
    "                return s.split()\n",
    "        \n",
    "        def to_sentences(self, s):\n",
    "                return [sent.strip() + '.' for sent in s.split('.') if len(sent.strip()) > 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbc523",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = custom_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648602d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prior_cases = \"Path to Citation DB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_suffixes = (\".txt\")\n",
    "citation_file_paths = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path_prior_cases):\n",
    "#     print(r,len(r))\n",
    "    for file in f:\n",
    "#         print(file)\n",
    "        if file.endswith(my_suffixes):\n",
    "            citation_file_paths.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {}\n",
    "corpus =[]\n",
    "citation_names = []\n",
    "for file in tqdm(sorted(citation_file_paths)):\n",
    "#     print(file)\n",
    "    filename = os.path.basename(file)\n",
    "#     print(filename)\n",
    "    f = codecs.open(file, \"r\", \"utf-8\", errors='ignore')\n",
    "    document = f.read()\n",
    "\n",
    "    refined_doc = re.split('\\n',document)\n",
    "    refined_doc = \"\\n\".join(refined_doc)\n",
    "    refined_doc = tokenizer.to_sentences(refined_doc.replace('\\n', ' '))\n",
    "\n",
    "    with open(\"directory path where you want to save\" + filename, 'w+') as output_file:\n",
    "        output_file.write(\"\\n\".join(refined_doc))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0bc9a",
   "metadata": {},
   "source": [
    "# For Query Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4499cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_query_cases = \"Path to Query DB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2015df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_suffixes = (\".txt\")\n",
    "citation_file_paths = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path_query_cases):\n",
    "#     print(r,len(r))\n",
    "    for file in f:\n",
    "#         print(file)\n",
    "        if file.endswith(my_suffixes):\n",
    "            citation_file_paths.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {}\n",
    "corpus =[]\n",
    "citation_names = []\n",
    "for file in tqdm(sorted(citation_file_paths)):\n",
    "#     print(file)\n",
    "    filename = os.path.basename(file)\n",
    "#     print(filename)\n",
    "    f = codecs.open(file, \"r\", \"utf-8\", errors='ignore')\n",
    "    document = f.read()\n",
    "\n",
    "    refined_doc = re.split('\\n',document)\n",
    "    \n",
    "    refined_doc = \"\\n\".join(refined_doc)\n",
    "    refined_doc = tokenizer.to_sentences(refined_doc.replace('\\n', ' '))\n",
    "\n",
    "    with open(\"directory path where you want to save\" + filename, 'w+') as output_file:\n",
    "        output_file.write(\"\\n\".join(refined_doc))\n",
    "#         output_file.write(refined_doc)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
