{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import progressbar\n",
    "import codecs\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.attrs import ORTH\n",
    "import re\n",
    "import string\n",
    "import pathlib \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk.data\n",
    "\n",
    "from time import time\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv \n",
    "from csv import reader\n",
    "import ast\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec033a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sentencizer(doc):\n",
    "    ''' Look for sentence start tokens by scanning for periods only. '''\n",
    "    split_lowercase = all(w.text.islower() for w in doc)\n",
    "    \n",
    "    for i, token in enumerate(doc[:-2]):  # The last token cannot start a sentence\n",
    "        if token.text[0] == \".\" or token.text[-1] == \".\":\n",
    "            if not split_lowercase and (not doc[i+1].text[0].isupper() or doc[i+2].text[0] == '.'):# or doc[i+1].text[0] == '.':\n",
    "                    doc[i+1].is_sent_start = False  # Tell the default sentencizer to ignore this token\n",
    "            # pass\n",
    "        else:\n",
    "            doc[i+1].is_sent_start = False  # Tell the default sentencizer to ignore this token\n",
    "    return doc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def custom_splitter(text = None):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    nlp.add_pipe(custom_sentencizer, before = \"parser\")\n",
    "    \n",
    "    special_cases = {\"Rs.\": \"rs.\", \"No.\": \"no.\", \"no.\": \"no.\", \"i.e.\": \"i.e.\", \"viz.\": \"viz.\", \"M/s.\": \"m/s.\", \"Mohd.\": \"mohd.\", \"Ex.\": \"exhibit\", \"Art.\" : \"article\", \"Arts.\" : \"articles\", \"S.\": \"section\", \"s.\": \"section\", \"ss.\": \"sections\", \"u/s.\": \"section\", \"u/ss.\": \"sections\", \"art.\": \"article\", \"arts.\": \"articles\", \"u/arts.\" : \"articles\", \"u/art.\" : \"article\", \"hon'ble\" : \"honourable\", \"ITO\" : \"Ito\", \"UBI\" : \"Ubi\", \"Ors.\" : \"ors.\"}    \n",
    "#     special_cases = {\"Rs.\": \"rs.\", \"No.\": \"no.\", \"no.\": \"no.\", \"v.\": \"vs\", \"vs.\": \"vs\", \"i.e.\": \"i.e.\", \"viz.\": \"viz.\", \"M/s.\": \"m/s.\", \"Mohd.\": \"mohd.\", \"Ex.\": \"exhibit\", \"Art.\" : \"article\", \"Arts.\" : \"articles\", \"S.\": \"section\", \"s.\": \"section\", \"ss.\": \"sections\", \"u/s.\": \"section\", \"u/ss.\": \"sections\", \"art.\": \"article\", \"arts.\": \"articles\", \"u/arts.\" : \"articles\", \"u/art.\" : \"article\", \"hon'ble\" : \"honourable\"}\n",
    "    #Ltd. Pvt. Corp.\n",
    "\n",
    "\n",
    "    for case, orth in special_cases.items():\n",
    "    \tnlp.tokenizer.add_special_case(case, [{ORTH: orth}])\n",
    "    \n",
    "    \n",
    "    if text is None: return nlp\n",
    "    #text = text.strip()\n",
    "    #print (text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    #text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    parsed = nlp(text)\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for sent in parsed.sents:\n",
    "        sentences.append(sent.text)\n",
    "    \n",
    "    return sentences, nlp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class custom_tokenizer:\n",
    "        def __init__(self):\n",
    "                # self.NLP = spacy.load('en_core_web_sm')\n",
    "                self.NLP = custom_splitter()\n",
    "                puncts = string.punctuation.replace('.', '').replace('-', '')\n",
    "                self.trans = str.maketrans('.-','  ', puncts)\n",
    "                \n",
    "        def to_words(self, text):\n",
    "                text = re.sub('\\n', ' ', text.lower())\n",
    "                text = re.sub('\\s+', ' ', text).strip()\n",
    "                \n",
    "                words = [s.text.lower() if s.text[0] == \"'\" and len(s.text) == 2 else s.text.translate(self.trans).strip().lower() for s in self.NLP(text.strip()) if not s.is_punct]\n",
    "                \n",
    "                return words \n",
    "        \n",
    "        def to_sentences(self, text):\n",
    "                #remove extra dots\n",
    "                text = re.sub('\\.\\s*\\.\\s*\\.', '. ', text)\n",
    "                text = re.sub('\\.\\s*\\.', '. ', text)\n",
    "                \n",
    "                #remove dash\n",
    "#                 text = re.sub('-', ' ', text)\n",
    "                \n",
    "                # remove extra whitespace\n",
    "                text = re.sub('\\n', ' ', text)\n",
    "                text = re.sub('\\s+', ' ', text).strip()\n",
    "                \n",
    "                \n",
    "                \n",
    "                sentences = [s.text for s in self.NLP(text).sents if len(s.text.strip()) > 5]\n",
    "                # if re.match('\\d+\\.?.*', text):\n",
    "                #         text = text[4:]\n",
    "                \n",
    "                return sentences\n",
    "        \n",
    "        def to_cleaned_sents(self, text):\n",
    "                sents = self.to_sentences(text)\n",
    "                words = [' '.join(self.to_words(s)) + '.' for s in sents]\n",
    "                return words        \n",
    "        \n",
    "       \n",
    "        \n",
    "       \n",
    "        \n",
    "class simple_tokenizer:\n",
    "        def to_words(self, s):\n",
    "                s = s.strip().strip('.').strip()\n",
    "                return s.split()\n",
    "        \n",
    "        def to_sentences(self, s):\n",
    "                return [sent.strip() + '.' for sent in s.split('.') if len(sent.strip()) > 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfedd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = custom_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implementation of OKapi BM25 with sklearn's TfidfVectorizer\n",
    "Distributed as CC-0 (https://creativecommons.org/publicdomain/zero/1.0/)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "    def __init__(self, b=0.7, k1=1.6):\n",
    "#         self.vectorizer = TfidfVectorizer(norm=None, smooth_idf=False)\n",
    "#         self.vectorizer = TfidfVectorizer(tokenizer=stemming_tokenizer, \n",
    "#                                           max_df=.90, min_df=1,\n",
    "#                                           stop_words='english', \n",
    "#                                           use_idf=True, \n",
    "#                                           ngram_range=(2, 2))\n",
    "        self.vectorizer = TfidfVectorizer(max_df=.65, min_df=1,\n",
    "                                  use_idf=True, \n",
    "                                  ngram_range=(1, 1))\n",
    "        \n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Fit IDF to documents X \"\"\"\n",
    "        self.vectorizer.fit(X)\n",
    "        y = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        self.avdl = y.sum(1).mean()\n",
    "\n",
    "    def transform(self, q, X):\n",
    "        \"\"\" Calculate BM25 between query q and documents X \"\"\"\n",
    "        b, k1, avdl = self.b, self.k1, self.avdl\n",
    "\n",
    "        # apply CountVectorizer\n",
    "        X = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        len_X = X.sum(1).A1\n",
    "        q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n",
    "        assert sparse.isspmatrix_csr(q)\n",
    "\n",
    "        # convert to csc for better column slicing\n",
    "        X = X.tocsc()[:, q.indices]\n",
    "        denom = X + (k1 * (1 - b + b * len_X / avdl))[:, None]\n",
    "        # idf(t) = log [ n / df(t) ] + 1 in sklearn, so it need to be coneverted\n",
    "        # to idf(t) = log [ n / df(t) ] with minus 1\n",
    "        idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n",
    "        numer = X.multiply(np.broadcast_to(idf, X.shape)) * (k1 + 1)                                                          \n",
    "        return (numer / denom).sum(1).A1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798e8e2",
   "metadata": {},
   "source": [
    "# create corpus for prior cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"local directory path for preprocessed citation Database\"\n",
    "\n",
    "db_path_list = []\n",
    "for p in pathlib.Path(db_path).iterdir():\n",
    "    db_path_list.append(p)\n",
    "\n",
    "dir_dict = {}\n",
    "\n",
    "for case_path in tqdm(sorted(db_path_list)):\n",
    "    if(os.path.basename(case_path) == \"__MACOSX\"):\n",
    "        continue;\n",
    "    for r,d,f in os.walk(case_path):\n",
    "        for file in f:\n",
    "            if(file==\"entailed_fragment.txt\"):\n",
    "                dir_dict[str(case_path/file)] = []\n",
    "            elif(file==\"base_case.txt\"):\n",
    "                continue;\n",
    "            else:\n",
    "                if(os.path.basename(case_path) == \".DS_Store\"):\n",
    "                    continue\n",
    "                if(file == \".DS_Store\"):\n",
    "                    continue\n",
    "                dir_dict[str(case_path/\"entailed_fragment.txt\")].append(str(case_path/\"paragraphs\"/file))\n",
    "                \n",
    "for k in dir_dict.keys():\n",
    "    dir_dict[k] = sorted(dir_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e1519",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "prediction_dict = {}\n",
    "count = 0\n",
    "final_pred = {} \n",
    "\n",
    "for case_path in tqdm(dir_dict.keys()):\n",
    "    #create query file\n",
    "    f = codecs.open(case_path, \"r\", \"utf-8\", errors='ignore')\n",
    "    q_case_text = f.read()\n",
    "#     print(case_path)\n",
    "#     #basic preprocessing\n",
    "#     raw_str_list = q_case_text.splitlines()\n",
    "    #basic preprocessing\n",
    "    raw_str_list = tokenizer.to_sentences(q_case_text.replace('\\n', ' '))\n",
    "    query_case = ''.join(raw_str_list)\n",
    "    \n",
    "    cite_corpus = []\n",
    "    citation_names = []\n",
    "    #create citation corpus for that case\n",
    "    for cite_path in dir_dict[case_path]:\n",
    "        cite_file = codecs.open(cite_path, \"r\", \"utf-8\", errors='ignore')\n",
    "        c_case_text = cite_file.read()\n",
    "        \n",
    "        #basic preprocessing\n",
    "        prepro_c_case_text = tokenizer.to_sentences(c_case_text.replace('\\n', ' '))\n",
    "        \n",
    "        citation_text = ''.join(prepro_c_case_text)\n",
    "        cite_corpus.append(citation_text)\n",
    "        citation_names.append(os.path.basename(cite_path))\n",
    "    \n",
    "    \n",
    "    bm25 = BM25()\n",
    "    bm25.fit(cite_corpus)\n",
    "    \n",
    "    qu = query_case\n",
    "    qu_n = os.path.dirname(case_path)\n",
    "    \n",
    "    doc_scores = bm25.transform(qu, cite_corpus)\n",
    "    score_dict[qu_n] = doc_scores\n",
    "    doc_sort_index = np.argsort(doc_scores)\n",
    "    do_sort_index_rev = doc_sort_index[::-1]\n",
    "    final_pred[os.path.basename(qu_n)] = [citation_names[case] for case in do_sort_index_rev]\n",
    "#     count += 1\n",
    "    del bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(list(zip(final_pred.keys(),final_pred.values())), columns = [\"Document id\", \"Prediction List\"])\n",
    "pred_df['Document id'] = pred_df['Document id'].apply(lambda x : os.path.basename(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65975e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STORE ACTUAL NUMBER OF CITATIONS IN DICTIONARY\n",
    "golden_citations = {}\n",
    "with open(\"Give path reference to Golden Citation JSON file for Task-2\", 'r') as actual_json:\n",
    "    golden_citations = json.load(actual_json)\n",
    "golden_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9597bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = 0\n",
    "c2 = 0\n",
    "c3 = 0\n",
    "c4 = 0\n",
    "for k in golden_citations.keys():\n",
    "    if len(golden_citations[k])== 1:\n",
    "        c1 +=1\n",
    "    if len(golden_citations[k])== 2:\n",
    "        c2 +=1\n",
    "    if len(golden_citations[k])== 3:\n",
    "        c3 +=1\n",
    "    if len(golden_citations[k])== 4:\n",
    "        c4 +=1\n",
    "print(len(golden_citations.keys()), c1, c2 , c3, c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80093bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 461 + 57*2 + 4*3 + 3*4\n",
    "avg = tot/525\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d245fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision@K Function\n",
    "def prec_at_k(true_list,pred_list,k):\n",
    "    #define list of top k predictions\n",
    "    count = 0\n",
    "    top_k_pred = pred_list[0:k].copy()\n",
    "    #iterate throught the top k predictions\n",
    "    for doc in top_k_pred:\n",
    "        #if document in true list, then increment count of relevant predictions\n",
    "        if doc in true_list:\n",
    "            count += 1\n",
    "    #return total_relevant_predictions_in_top_k/k\n",
    "    return count/k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall@K Function\n",
    "def recall_at_k(true_list,pred_list,k,r):\n",
    "    #define top k predictions\n",
    "    count=0\n",
    "    top_k_pred = pred_list[0:k].copy()\n",
    "    #iterate through the top k predictions\n",
    "    for doc in top_k_pred:\n",
    "        #if doc in true list, then increment count\n",
    "        if doc in true_list:\n",
    "            count+=1\n",
    "    #return number of relevant documents in top k predictions/total number of relevant predictions\n",
    "    return count/r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b68b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Precision Function\n",
    "def AP(true_list,pred_list):\n",
    "    #P-> relative precision list, rel_vec-> relevance vector \n",
    "    P = []\n",
    "    rel_vec = []\n",
    "    val = 0 \n",
    "    #iterate through the entire prediction list \n",
    "    for i in range(len(pred_list)):\n",
    "        #if predicted citation in true list increment numberator (number of relevant docs) by 1 and also append 1 for rel_vec\n",
    "        if pred_list[i] in true_list:\n",
    "            val += 1\n",
    "            rel_vec.append(1)\n",
    "        else:\n",
    "            #otherwise just append 0 for rel_vec\n",
    "            rel_vec.append(0)\n",
    "        #append the relative precision for each query document while iterating\n",
    "        # so append (number of relevant docs so far ie., val) divided by total number of documents iterated so far\n",
    "        P.append(val/(i+1))\n",
    "    count = 0\n",
    "    total = 0\n",
    "    #find the relatve precision of all the relevant documents and take sum\n",
    "    for rank in range(len(P)):\n",
    "        # for index in P list\n",
    "        # if rel_vec[i] is 1 that means it is relevant document thus increment count and add to total, else dont count\n",
    "        if rel_vec[rank] == 1:\n",
    "            count += 1\n",
    "            total += P[rank]\n",
    "    # boundary case where there is no relevent document found\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    #return the Average Precision\n",
    "    return total/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reciprocal Rank Function\n",
    "def RR(true_list,pred_list):\n",
    "    #iterate through the ranked prediction list, break at first relevant case and return reciprocal of that rank\n",
    "    for i in range(len(pred_list)):\n",
    "        if pred_list[i] in true_list:\n",
    "            return 1/(i+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5ed67",
   "metadata": {},
   "source": [
    "## Get all the results based on the Golden citation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee86d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dataframe for results\n",
    "results = pd.DataFrame(columns=['Document id','Prec@1','Prec@5','Prec@10','Prec@R','Recall@100','AP','RR'])\n",
    "for i in tqdm(pred_df.index):\n",
    "    #fetch the details from prediction dataframe\n",
    "    query_case = pred_df.iloc[i,0]\n",
    "#     print(query_case)\n",
    "    #r = pred_df.iloc[i,1]\n",
    "    #true_list = pred_df.iloc[i,3].copy()\n",
    "    true_list = golden_citations.get(query_case)\n",
    "    r = len(true_list)\n",
    "    #pred_list = pred_df.iloc[i,4].copy()\n",
    "    pred_list = pred_df.iloc[i,1].copy()\n",
    "    prec_at_1 = prec_at_k(true_list,pred_list,1)\n",
    "    prec_at_5 = prec_at_k(true_list,pred_list,5)\n",
    "    prec_at_10 = prec_at_k(true_list,pred_list,10)\n",
    "    prec_at_r = prec_at_k(true_list,pred_list,r)\n",
    "    \n",
    "    recall_at_100 = recall_at_k(true_list,pred_list,100,r)\n",
    "    ap = AP(true_list,pred_list)\n",
    "    rr = RR(true_list,pred_list)\n",
    "    #add the details to the result dataframe\n",
    "    results = results.append({'Document id':query_case, 'Prec@1': prec_at_1, 'Prec@5': prec_at_5 , 'Prec@10': prec_at_10, 'Prec@R': prec_at_r, 'Recall@100': recall_at_100, 'AP': ap, 'RR': rr}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179214ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(results.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97550fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Micro Precision Function\n",
    "def micro_prec(true_list,pred_list,k):\n",
    "    #define list of top k predictions\n",
    "    cor_pred = 0\n",
    "    top_k_pred = pred_list[0:k].copy()\n",
    "    #iterate throught the top k predictions\n",
    "    for doc in top_k_pred:\n",
    "        #if document in true list, then increment count of relevant predictions\n",
    "        if doc in true_list:\n",
    "            cor_pred += 1\n",
    "    #return total_relevant_predictions_in_top_k/k\n",
    "    return cor_pred, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04dab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dataframe for results\n",
    "results = pd.DataFrame(columns=['Document id','Prec@1','Prec@5','Prec@10','Prec@R','Recall@100','AP','RR','Correct_pred','Retrived_cases', 'Relevant_cases'])\n",
    "correct_pred = 0\n",
    "retri_cases = 0\n",
    "relevant_cases = 0\n",
    "\n",
    "for i in tqdm(pred_df.index):\n",
    "    #fetch the details from prediction dataframe\n",
    "    query_case = pred_df.iloc[i,0]\n",
    "#     print(query_case)\n",
    "    #r = pred_df.iloc[i,1]\n",
    "    #true_list = pred_df.iloc[i,3].copy()\n",
    "    true_list = golden_citations.get(query_case)\n",
    "#     print(query_case, type(query_case))\n",
    "\n",
    "    r = 1 # for constant predictions for each query case\n",
    "#     r = len(true_list)\n",
    "    pred_list = pred_df.iloc[i,1].copy()\n",
    "    prec_at_1 = prec_at_k(true_list,pred_list,1)\n",
    "    prec_at_5 = prec_at_k(true_list,pred_list,5)\n",
    "    prec_at_10 = prec_at_k(true_list,pred_list,10)\n",
    "    prec_at_r = prec_at_k(true_list,pred_list,r)\n",
    "    \n",
    "    c_p, r_c = micro_prec(true_list,pred_list,r)\n",
    "    correct_pred += c_p\n",
    "    retri_cases += r_c\n",
    "    relevant_cases += len(true_list)\n",
    "    \n",
    "    recall_at_100 = recall_at_k(true_list,pred_list,100,r)\n",
    "    ap = AP(true_list,pred_list)\n",
    "    rr = RR(true_list,pred_list)\n",
    "    #add the details to the result dataframe\n",
    "    results = results.append({'Document id':query_case, 'Prec@1': prec_at_1, 'Prec@5': prec_at_5 , 'Prec@10': prec_at_10, 'Prec@R': prec_at_r, 'Recall@100': recall_at_100, 'AP': ap, 'RR': rr, 'Correct_pred':c_p, 'Retrived_cases':r_c, 'Relevant_cases':len(true_list)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c81f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f81075",
   "metadata": {},
   "outputs": [],
   "source": [
    "(results.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correct Predictions: \", correct_pred)\n",
    "print(\"Retrived Cases Predictions: \", retri_cases)\n",
    "print(\"Relevant Cases: \", relevant_cases)\n",
    "\n",
    "M_pre = correct_pred/retri_cases\n",
    "M_recall = correct_pred/relevant_cases\n",
    "M_F = 2*M_pre*M_recall/ (M_pre + M_recall)\n",
    "\n",
    "print(\"Micro Precision: \", M_pre)\n",
    "print(\"Micro Recall: \", M_recall)\n",
    "print(\"Micro F-Measure: \", M_F)\n",
    "print(correct_pred, \"\\t\", retri_cases, \"\\t\", relevant_cases, \"\\t\", M_pre, \"\\t\", M_recall, \"\\t\", M_F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
