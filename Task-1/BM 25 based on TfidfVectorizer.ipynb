{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import progressbar\n",
    "import codecs\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.attrs import ORTH\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk.data\n",
    "\n",
    "from time import time\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv \n",
    "from csv import reader\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prior_cases = \"local directory path for preprocessed citation Database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51134473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "import re # regular expression\n",
    "\n",
    "\n",
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z\\-]\", \" \", str_input).lower().split() # delete non letter charactors\n",
    "    #words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split() # include numbers\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138128f",
   "metadata": {},
   "source": [
    "Reference: To check and update TfidfVectorizer [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implementation of OKapi BM25 with sklearn's TfidfVectorizer\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "    def __init__(self, b=0.7, k1=1.6):\n",
    "#         self.vectorizer = TfidfVectorizer(tokenizer=stemming_tokenizer, \n",
    "#                                           max_df=.90, min_df=1,\n",
    "#                                           stop_words='english', \n",
    "#                                           use_idf=True, \n",
    "#                                           ngram_range=(2, 2))\n",
    "        self.vectorizer = TfidfVectorizer(max_df=.65, min_df=1,\n",
    "                                  use_idf=True, \n",
    "                                  ngram_range=(1, 1))\n",
    "        \n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Fit IDF to documents X \"\"\"\n",
    "        self.vectorizer.fit(X)\n",
    "        y = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        self.avdl = y.sum(1).mean()\n",
    "\n",
    "    def transform(self, q, X):\n",
    "        \"\"\" Calculate BM25 between query q and documents X \"\"\"\n",
    "        b, k1, avdl = self.b, self.k1, self.avdl\n",
    "\n",
    "        # apply CountVectorizer\n",
    "        X = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        len_X = X.sum(1).A1\n",
    "        q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n",
    "        assert sparse.isspmatrix_csr(q)\n",
    "\n",
    "        # convert to csc for better column slicing\n",
    "        X = X.tocsc()[:, q.indices]\n",
    "        denom = X + (k1 * (1 - b + b * len_X / avdl))[:, None]\n",
    "        # idf(t) = log [ n / df(t) ] + 1 in sklearn, so it need to be coneverted\n",
    "        # to idf(t) = log [ n / df(t) ] with minus 1\n",
    "        idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n",
    "        numer = X.multiply(np.broadcast_to(idf, X.shape)) * (k1 + 1)                                                          \n",
    "        return (numer / denom).sum(1).A1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798e8e2",
   "metadata": {},
   "source": [
    "# create corpus for prior cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_suffixes = (\".txt\")\n",
    "citation_file_paths = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path_prior_cases):\n",
    "#     print(r,len(r))\n",
    "    for file in f:\n",
    "#         print(file)\n",
    "        if file.endswith(my_suffixes):\n",
    "            citation_file_paths.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {}\n",
    "corpus =[]\n",
    "citation_names = []\n",
    "for file in sorted(citation_file_paths):\n",
    "#     print(file)\n",
    "    f = codecs.open(file, \"r\", \"utf-8\", errors='ignore')\n",
    "    text = f.read()\n",
    "    corpus.append(text)\n",
    "    citation_names.append(os.path.basename(file))\n",
    "    name_dict[text] = os.path.basename(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc3beb",
   "metadata": {},
   "source": [
    "# create a query corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_current_cases = \"local directory path for preprocessed Query Database\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634fcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_suffixes = (\".txt\")\n",
    "query_file_paths = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path_current_cases):\n",
    "#     print(r,len(r))\n",
    "    for file in f:\n",
    "#         print(file)\n",
    "        if file.endswith(my_suffixes):\n",
    "            query_file_paths.append(os.path.join(r, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb32fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_corpus = []\n",
    "query_names = [] \n",
    "\n",
    "#iterate throught the query database list in sorted manner\n",
    "for file in tqdm(sorted(query_file_paths),desc = \"query documents\"):\n",
    "    #open the file -> read the file -> split the lines\n",
    "    open_file = open(file, 'r', encoding=\"utf-8\")\n",
    "    text = open_file.read()\n",
    "    \n",
    "    raw_str_list = text.splitlines()\n",
    "    #Append short length sentences to previous sentence. (to avoid wrongly splitted sentences)\n",
    "    str_list = []\n",
    "    #if first sentence is very short append it at start of 2nd sentence and start from 3rd\n",
    "    if len(raw_str_list[0])<100:\n",
    "        start_ind = 2\n",
    "        str_list.append(raw_str_list[0] + \" \" + raw_str_list[1])\n",
    "    else:\n",
    "        start_ind = 1\n",
    "        str_list.append(raw_str_list[0])\n",
    "    \n",
    "    for line in raw_str_list[start_ind:]:\n",
    "        #if phrase/line is less than 100 characters, then we append it to the previous line\n",
    "        if len(line)<101:\n",
    "            str_list[-1] += \" \"\n",
    "            str_list[-1] += line\n",
    "            continue\n",
    "        else:\n",
    "            str_list.append(line)\n",
    "    \n",
    "    str_list_3 = []\n",
    "    \n",
    "#     print(file)\n",
    "    for i in range(len(str_list)):\n",
    "        if \"CITATION_SUPPRESSED\" in str_list[i] or \"FRAGMENT_SUPPRESSED\" in str_list[i] or \"REFERENCE_SUPPRESSED\" in str_list[i]:\n",
    "            str_list_3 += str_list[max(0,i-3):i+4] # consider three previous and only three next sentences\n",
    "    \n",
    "#     print(\"\".join(str_list_3))\n",
    "    query_corpus.append(''.join(str_list_3))\n",
    "    #query_corpus += str_list_3\n",
    "#     break\n",
    "    \n",
    "    query_names.append(os.path.basename(file))\n",
    "    \n",
    "    #close the query file\n",
    "    open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2650a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STORE ACTUAL NUMBER OF CITATIONS IN DICTIONARY\n",
    "golden_citations = {}\n",
    "golden = {}\n",
    "\n",
    "\n",
    "with open(\"Give path reference to Golden Citation CSV file for Task-1\",'r') as actual_csv:\n",
    "    #read the csv and iterate through it\n",
    "    a = reader(actual_csv)\n",
    "    for row in a:\n",
    "        if row[1] == 'current case':\n",
    "            continue\n",
    "#         print(row)\n",
    "        #for each row, store the number of citations and true list of citations\n",
    "        golden[row[1]] = int(row[2])\n",
    "        golden_citations[row[1]] = ast.literal_eval(row[3])\n",
    "actual_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0577724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e166d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25()\n",
    "bm25.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54718a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "prediction_dict = {}\n",
    "pred_df = pd.DataFrame(columns=['Documend id','No of Golden Citations','Min BM25 Sim Value in TOP R','Actual Citations','Prediction List'])\n",
    "\n",
    "# tokenized_query = [doc.split(\" \") for doc in tqdm(query_corpus)]\n",
    "for i in tqdm(range(len(query_corpus))):\n",
    "    qu = query_corpus[i]\n",
    "    qu_n = query_names[i]\n",
    "    \n",
    "    R = golden[qu_n]\n",
    "#     print(qu_n,R)\n",
    "    \n",
    "    doc_scores = bm25.transform(qu, corpus)\n",
    "    rev_doc_score = sorted(doc_scores, reverse=True)\n",
    "    score_dict[qu_n] = doc_scores\n",
    "    doc_sort_index = np.argsort(doc_scores)\n",
    "    do_sort_index_rev = doc_sort_index[::-1]\n",
    "    prediction_dict[qu_n] = do_sort_index_rev\n",
    "    \n",
    "    min_tup = rev_doc_score[R-1]\n",
    "#     print(min_tup)\n",
    "    \n",
    "    predictions = [citation_names[case] for case in prediction_dict[qu_n]]\n",
    "    \n",
    "#     print(predictions)\n",
    "    pred_df = pred_df.append({'Documend id':qu_n,'No of Golden Citations':R,'Min BM25 Sim Value in TOP R':min_tup,'Actual Citations':golden_citations.get(qu_n),'Prediction List':predictions},ignore_index=True)\n",
    "#     print(qu_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b9ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"Give path to save the prediction CSV file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create rsults directory\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d245fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision@K Function\n",
    "def prec_at_k(true_list,pred_list,k):\n",
    "    #define list of top k predictions\n",
    "    count = 0\n",
    "    top_k_pred = pred_list[0:k].copy()\n",
    "    #iterate throught the top k predictions\n",
    "    for doc in top_k_pred:\n",
    "        #if document in true list, then increment count of relevant predictions\n",
    "        if doc in true_list:\n",
    "            count += 1\n",
    "    #return total_relevant_predictions_in_top_k/k\n",
    "    return count/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall@K Function\n",
    "def recall_at_k(true_list,pred_list,k,r):\n",
    "    #define top k predictions\n",
    "    count=0\n",
    "    top_k_pred = pred_list[0:k].copy()\n",
    "    #iterate through the top k predictions\n",
    "    for doc in top_k_pred:\n",
    "        #if doc in true list, then increment count\n",
    "        if doc in true_list:\n",
    "            count+=1\n",
    "    #return number of relevant documents in top k predictions/total number of relevant predictions\n",
    "    return count/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b68b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Precision Function\n",
    "def AP(true_list,pred_list):\n",
    "    #P-> relative precision list, rel_vec-> relevance vector \n",
    "    P = []\n",
    "    rel_vec = []\n",
    "    val = 0 \n",
    "    #iterate through the entire prediction list \n",
    "    for i in range(len(pred_list)):\n",
    "        #if predicted citation in true list increment numberator (number of relevant docs) by 1 and also append 1 for rel_vec\n",
    "        if pred_list[i] in true_list:\n",
    "            val += 1\n",
    "            rel_vec.append(1)\n",
    "        else:\n",
    "            #otherwise just append 0 for rel_vec\n",
    "            rel_vec.append(0)\n",
    "        #append the relative precision for each query document while iterating\n",
    "        # so append (number of relevant docs so far ie., val) divided by total number of documents iterated so far\n",
    "        P.append(val/(i+1))\n",
    "    count = 0\n",
    "    total = 0\n",
    "    #find the relatve precision of all the relevant documents and take sum\n",
    "    for rank in range(len(P)):\n",
    "        # for index in P list\n",
    "        # if rel_vec[i] is 1 that means it is relevant document thus increment count and add to total, else dont count\n",
    "        if rel_vec[rank] == 1:\n",
    "            count += 1\n",
    "            total += P[rank]\n",
    "    # boundary case where there is no relevent document found\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    #return the Average Precision\n",
    "    return total/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reciprocal Rank Function\n",
    "def RR(true_list,pred_list):\n",
    "    #iterate through the ranked prediction list, break at first relevant case and return reciprocal of that rank\n",
    "    for i in range(len(pred_list)):\n",
    "        if pred_list[i] in true_list:\n",
    "            return 1/(i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4907c",
   "metadata": {},
   "source": [
    "## Get all the results based on the Golden citation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dataframe for results\n",
    "results = pd.DataFrame(columns=['Document id','Prec@1','Prec@5','Prec@10','Prec@R','Recall@100','AP','RR'])\n",
    "for i in tqdm(pred_df.index):\n",
    "    #fetch the details from prediction dataframe\n",
    "    query_case = pred_df.iloc[i,0]\n",
    "#     print(query_case)\n",
    "    #r = pred_df.iloc[i,1]\n",
    "    #true_list = pred_df.iloc[i,3].copy()\n",
    "    true_list = golden_citations.get(query_case)\n",
    "    r = len(true_list)\n",
    "    #pred_list = pred_df.iloc[i,4].copy()\n",
    "    pred_list = pred_df.iloc[i,4].copy()\n",
    "    prec_at_1 = prec_at_k(true_list,pred_list,1)\n",
    "    prec_at_5 = prec_at_k(true_list,pred_list,5)\n",
    "    prec_at_10 = prec_at_k(true_list,pred_list,10)\n",
    "    prec_at_r = prec_at_k(true_list,pred_list,r)\n",
    "    \n",
    "    recall_at_100 = recall_at_k(true_list,pred_list,100,r)\n",
    "    ap = AP(true_list,pred_list)\n",
    "    rr = RR(true_list,pred_list)\n",
    "    #add the details to the result dataframe\n",
    "    results = results.append({'Document id':query_case, 'Prec@1': prec_at_1, 'Prec@5': prec_at_5 , 'Prec@10': prec_at_10, 'Prec@R': prec_at_r, 'Recall@100': recall_at_100, 'AP': ap, 'RR': rr}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e3451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(results[results['Recall@100']<1]) #print number of rows whose recall score is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6467f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cwd = os.getcwd()\n",
    "results.to_csv(\"save the results in CSV file for future reference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179214ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470588e9",
   "metadata": {},
   "source": [
    "Include segment csv path contains which file have:\n",
    "* Document ID\t\n",
    "* FRAGMENT_SUPPRESSED\n",
    "* REFERENCE_SUPPRESSED\n",
    "* CITATION_SUPPRESSED\n",
    "* Golden_Citations\n",
    "* Difference (#FRAGMENT_SUPPRESSED + #REFERENCE_SUPPRESSED - #Golden_Citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf838b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_csv_path = \"path to the segment CSV file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97550fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segment_csv = pd.read_csv(segment_csv_path, index_col=0)\n",
    "df_segment_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Micro Precision Function\n",
    "def micro_prec(true_list,pred_list,k):\n",
    "    #define list of top k predictions\n",
    "    cor_pred = 0\n",
    "    top_k_pred = pred_list[0:k].copy()\n",
    "    #iterate throught the top k predictions\n",
    "    for doc in top_k_pred:\n",
    "        #if document in true list, then increment count of relevant predictions\n",
    "        if doc in true_list:\n",
    "            cor_pred += 1\n",
    "    #return total_relevant_predictions_in_top_k/k\n",
    "    return cor_pred, k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ce512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dataframe for results\n",
    "results = pd.DataFrame(columns=['Document id','Prec@1','Prec@5','Prec@10','Prec@R','Recall@100','AP','RR','Correct_pred','Retrived_cases', 'Relevant_cases'])\n",
    "correct_pred = 0\n",
    "retri_cases = 0\n",
    "relevant_cases = 0\n",
    "\n",
    "for i in tqdm(pred_df.index):\n",
    "    #fetch the details from prediction dataframe\n",
    "    query_case = pred_df.iloc[i,0]\n",
    "#     print(query_case)\n",
    "    #r = pred_df.iloc[i,1]\n",
    "    #true_list = pred_df.iloc[i,3].copy()\n",
    "    true_list = golden_citations.get(query_case)\n",
    "#     print(query_case, type(query_case))\n",
    "    nf_FS = df_segment_csv.loc[df_segment_csv[\"Document ID\"]==query_case, \"#FRAGMENT_SUPPRESSED\"].iloc[0]\n",
    "    nf_RS = df_segment_csv.loc[df_segment_csv[\"Document ID\"]==query_case, \"#REFERENCE_SUPPRESSED\"].iloc[0]\n",
    "    \n",
    "# predict based on the number of FRAGMENT_SUPPRESSED or REFERENCE_SUPPRESSED mentioned in the query documents\n",
    "\n",
    "#     if nf_FS > 0:\n",
    "#         r = nf_FS\n",
    "#     else:\n",
    "#         r = nf_RS\n",
    "#     if r > 20:\n",
    "#         r = int(r/5)\n",
    "\n",
    "    r = 5 # for constant predictions for each query case\n",
    "    \n",
    "#     r = len(true_list) #predict based on actual predictions based on given golden citation file\n",
    "    #pred_list = pred_df.iloc[i,4].copy()\n",
    "#     c_p = 0\n",
    "#     r_c = 0\n",
    "    pred_list = pred_df.iloc[i,4].copy()\n",
    "    prec_at_1 = prec_at_k(true_list,pred_list,1)\n",
    "    prec_at_5 = prec_at_k(true_list,pred_list,5)\n",
    "    prec_at_10 = prec_at_k(true_list,pred_list,10)\n",
    "    prec_at_r = prec_at_k(true_list,pred_list,r)\n",
    "    \n",
    "    c_p, r_c = micro_prec(true_list,pred_list,r)\n",
    "    correct_pred += c_p\n",
    "    retri_cases += r_c\n",
    "    relevant_cases += len(true_list)\n",
    "    \n",
    "    recall_at_100 = recall_at_k(true_list,pred_list,100,r)\n",
    "    ap = AP(true_list,pred_list)\n",
    "    rr = RR(true_list,pred_list)\n",
    "    #add the details to the result dataframe\n",
    "    results = results.append({'Document id':query_case, 'Prec@1': prec_at_1, 'Prec@5': prec_at_5 , 'Prec@10': prec_at_10, 'Prec@R': prec_at_r, 'Recall@100': recall_at_100, 'AP': ap, 'RR': rr, 'Correct_pred':c_p, 'Retrived_cases':r_c, 'Relevant_cases':len(true_list)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9801f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correct Predictions: \", correct_pred)\n",
    "print(\"Retrived Cases Predictions: \", retri_cases)\n",
    "print(\"Relevant Cases: \", relevant_cases)\n",
    "\n",
    "M_pre = correct_pred/retri_cases\n",
    "M_recall = correct_pred/relevant_cases\n",
    "M_F = 2*M_pre*M_recall/ (M_pre + M_recall)\n",
    "\n",
    "print(\"Micro Precision: \", M_pre)\n",
    "print(\"Micro Recall: \", M_recall)\n",
    "print(\"Micro F-Measure: \", M_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bdd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_df[pred_df[\"Min BM25 Sim Value in TOP R\"]<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece19a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[\"Min BM25 Sim Value in TOP R\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2943bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(pred_df[\"Min BM25 Sim Value in TOP R\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9c64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e839e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61db88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
