{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56466a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import shutil\n",
    "from tqdm import tqdm \n",
    "import csv\n",
    "from csv import reader\n",
    "import json\n",
    "import ast\n",
    "import sys\n",
    "import sent2vec\n",
    "import datetime\n",
    "\n",
    "import sklearn \n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95813ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf20788",
   "metadata": {},
   "source": [
    "## [Sent2vec](https://github.com/epfml/sent2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Model\n",
    "\n",
    "Model_path = \"path to the pretrained sent2vec model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2vec_model = sent2vec.Sent2vecModel()\n",
    "sent2vec_model.load_model(Model_path)\n",
    "\n",
    "print(\"*********\"*8)\n",
    "print(sent2vec_model)\n",
    "print(\"Loading successful...\")\n",
    "print(\"*********\"*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d218684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE QUERY DB\n",
    "#define the path for query database\n",
    "path = \"local directory path for preprocessed Query Database\"\n",
    "#create dataframe for storing the average vectors\n",
    "query_df = pd.DataFrame(columns=['Document ID','Document Vector'])\n",
    "#create a list of all the paths for the documents in the query database\n",
    "path_list = []\n",
    "for p in pathlib.Path(path).iterdir():\n",
    "    path_list.append(p)\n",
    "    \n",
    "\n",
    "#iterate throught the query database list in sorted manner\n",
    "for file in tqdm(sorted(path_list),desc = \"query documents\"):\n",
    "    #open the file -> read the file -> split the lines\n",
    "    open_file = open(file, 'r', encoding=\"utf-8\")\n",
    "    text = open_file.read()\n",
    "    \n",
    "    raw_str_list = text.splitlines()\n",
    "    #Append short length sentences to previous sentence. (to avoid wrongly splitted sentences)\n",
    "    str_list = []\n",
    "    #if first sentence is very short append it at start of 2nd sentence and start from 3rd\n",
    "    if len(raw_str_list[0])<100:\n",
    "        start_ind = 2\n",
    "        str_list.append(raw_str_list[0] + \" \" + raw_str_list[1])\n",
    "    else:\n",
    "        start_ind = 1\n",
    "        str_list.append(raw_str_list[0])\n",
    "    \n",
    "    for line in raw_str_list[start_ind:]:\n",
    "        #if phrase/line is less than 100 characters, then we append it to the previous line\n",
    "        if len(line)<101:\n",
    "            str_list[-1] += \" \"\n",
    "            str_list[-1] += line\n",
    "            continue\n",
    "        else:\n",
    "            str_list.append(line)\n",
    "    \n",
    "    str_list_3 = []\n",
    "    \n",
    "#     print(file)\n",
    "    for i in range(len(str_list)):\n",
    "        if \"CITATION_SUPPRESSED\" in str_list[i] or \"FRAGMENT_SUPPRESSED\" in str_list[i] or \"REFERENCE_SUPPRESSED\" in str_list[i]:\n",
    "            str_list_3 += str_list[max(0,i-3):i+4] # consider three previous and only three next sentences\n",
    "    \n",
    "    sent_vec_list = []\n",
    "    # encode the sentences using model\n",
    "    for line in str_list_3:\n",
    "        emb = sent2vec_model.embed_sentence(line).flatten().tolist()\n",
    "        sent_vec_list.append(np.array(emb))\n",
    "    \n",
    "    #add the document name and its vector into the dataframe\n",
    "    query_df = query_df.append({'Document ID':os.path.basename(file),'Document Vector':sent_vec_list},ignore_index=True)\n",
    "    #close the query file\n",
    "    open_file.close()\n",
    "    del raw_str_list, str_list, sent_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT citation DB\n",
    "#define the path for citation database\n",
    "path = \"local directory path for preprocessed citation Database\"\n",
    "#create dataframe for storing the average vectors\n",
    "citation_df = pd.DataFrame(columns=['Document ID','Document Vector'])\n",
    "#create a list of all the paths for the documents in the citation database\n",
    "path_list = []\n",
    "for p in pathlib.Path(path).iterdir():\n",
    "    path_list.append(p)\n",
    "\n",
    "#iterate throught the citation database list in sorted manner\n",
    "for file in tqdm(sorted(path_list),desc = \"avg cite vect\"):\n",
    "    #open the file -> read the file -> split the lines\n",
    "    open_file = open(file, 'r', encoding=\"utf-8\")\n",
    "    f = open_file.read()\n",
    "    raw_str_list = f.splitlines()\n",
    "    #Append short length sentences to previous sentence. (to avoid wrongly splitted sentences)\n",
    "    str_list = []\n",
    "    #if first sentence is very short append it at start of 2nd sentence and start from 3rd\n",
    "    if len(raw_str_list[0])<100:\n",
    "        start_ind = 2\n",
    "        str_list.append(raw_str_list[0] + \" \" + raw_str_list[1])\n",
    "    else:\n",
    "        start_ind = 1\n",
    "        str_list.append(raw_str_list[0])\n",
    "        \n",
    "    for line in raw_str_list[start_ind:]:\n",
    "        #if phrase/line is less than 100 characters, then we append it to the previous line\n",
    "        if len(line)<101:\n",
    "            str_list[-1] += \" \"\n",
    "            str_list[-1] += line\n",
    "            continue\n",
    "        else:\n",
    "            str_list.append(line)\n",
    "    sent_vec_list = []\n",
    "    # encode the sentences using model\n",
    "    for line in str_list:\n",
    "        emb = sent2vec_model.embed_sentence(line).flatten().tolist()\n",
    "        sent_vec_list.append(np.asarray(emb))\n",
    "    #add the document name and its vector into the dataframe\n",
    "    citation_df = citation_df.append({'Document ID':os.path.basename(file),'Document Vector':sent_vec_list},ignore_index=True)\n",
    "    #close the citation document\n",
    "    open_file.close()\n",
    "#     del raw_str_list, str_list, sent_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a048da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STORE ACTUAL NUMBER OF CITATIONS IN DICTIONARY\n",
    "golden_citations = {}\n",
    "golden = {}\n",
    "\n",
    "with open(\"Give path reference to Golden Citation CSV file for Task-1\",'r') as actual_csv:\n",
    "    #read the csv and iterate through it\n",
    "    a = reader(actual_csv)\n",
    "    for row in a:\n",
    "        if row[1] == 'current case':\n",
    "            continue\n",
    "#         print(row)\n",
    "        #for each row, store the number of citations and true list of citations\n",
    "        golden[row[1]] = int(row[2])\n",
    "        golden_citations[row[1]] = ast.literal_eval(row[3])\n",
    "actual_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289983c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(x,y):\n",
    "    k = cosine_similarity(x,y)\n",
    "    return k.max(axis=1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd06829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicitons\n",
    "#define the dataframe for storing the predictions\n",
    "pred_df = pd.DataFrame(columns=['Documend id','No of Golden Citations','Min Cosine Sim Value in TOP R','Actual Citations','Prediction List'])\n",
    "nan_score_files = []\n",
    "\n",
    "#iterate through the document vector dataframe\n",
    "for q_ind in tqdm(query_df.index):\n",
    "    #define the query case using iloc\n",
    "\n",
    "    query_case = query_df.iloc[q_ind,0]\n",
    "    #define the query vector\n",
    "    query_vec = np.asarray(query_df.iloc[q_ind,1])\n",
    "    #i-> count of citations, pred_tup_list-> list of tuples of cosine values and citation name, predictions->final ranked predictions\n",
    "    i = 0\n",
    "    pred_tup_list = []\n",
    "    predictions = []\n",
    "    #get the true number of citation as R \n",
    "    R = golden[query_case]\n",
    "    #iterate through the citation dataframe\n",
    "    for cite_case in search_space_dict[query_case]:\n",
    "        #define the citation document name and document vector\n",
    "#         cite_case = citation_df.iloc[c_ind,0]\n",
    "        c_ind = citation_df.index[citation_df[\"Document ID\"] == cite_case][0]\n",
    "        cite_vec = np.asarray(citation_df.iloc[c_ind,1])\n",
    "        #find the cosine similarity value\n",
    "        #print(\"Calculate Score between\", query_case, \" --> \", cite_case)\n",
    "        score = get_score(query_vec, cite_vec)\n",
    "        \n",
    "#         score = cos_sim_matrix(query_vec, cite_vec)\n",
    "        \n",
    "        if np.isnan(score):\n",
    "            #print(query_case)\n",
    "            nan_score_files.append(query_case)\n",
    "            \n",
    "        #skip if cosine sim value == 1 (same document)\n",
    "        if score != 1.0:\n",
    "            #increase the count of citations\n",
    "            i += 1\n",
    "            #add the tup to pred_tup_list\n",
    "            pred_tup_list.append((score,cite_case))\n",
    "        del cite_vec\n",
    "    #sort the pred_tup_list based on the cosine similarity values\n",
    "    pred_tup_list_sorted = sorted(pred_tup_list,key = lambda x: x[0],reverse=True)\n",
    "    \n",
    "    #find the min similarity value amongst the top R citations predicted\n",
    "    min_tup = pred_tup_list_sorted[R-1]\n",
    "    #iterate through list of tuples and get the citation names in sorted manner\n",
    "    for tup in pred_tup_list_sorted:\n",
    "        predictions.append(tup[1])\n",
    "    #add the necessary details to the dataframe\n",
    "    pred_df = pred_df.append({'Documend id':query_case,'No of Golden Citations':R,'Min Cosine Sim Value in TOP R':min_tup[0],'Actual Citations':golden_citations.get(query_case),'Prediction List':predictions},ignore_index=True)\n",
    "    del pred_tup_list, predictions, query_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f557719",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(nan_score_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"Give path to save the prediction CSV file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67117f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision@K Function\n",
    "def prec_at_k(true_list,pred_list,k):\n",
    "    #define list of top k predictions\n",
    "    count = 0\n",
    "    top_k_pred = pred_list[0:k].copy()\n",
    "    #iterate throught the top k predictions\n",
    "    for doc in top_k_pred:\n",
    "        #if document in true list, then increment count of relevant predictions\n",
    "        if doc in true_list:\n",
    "            count += 1\n",
    "    #return total_relevant_predictions_in_top_k/k\n",
    "    return count/k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017876c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall@K Function\n",
    "def recall_at_k(true_list,pred_list,k,r):\n",
    "    #define top k predictions\n",
    "    count=0\n",
    "    top_k_pred = pred_list[0:k].copy()\n",
    "    #iterate through the top k predictions\n",
    "    for doc in top_k_pred:\n",
    "        #if doc in true list, then increment count\n",
    "        if doc in true_list:\n",
    "            count+=1\n",
    "    #return number of relevant documents in top k predictions/total number of relevant predictions\n",
    "    return count/r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Precision Function\n",
    "def AP(true_list,pred_list):\n",
    "    #P-> relative precision list, rel_vec-> relevance vector \n",
    "    P = []\n",
    "    rel_vec = []\n",
    "    val = 0 \n",
    "    #iterate through the entire prediction list \n",
    "    for i in range(len(pred_list)):\n",
    "        #if predicted citation in true list increment numberator (number of relevant docs) by 1 and also append 1 for rel_vec\n",
    "        if pred_list[i] in true_list:\n",
    "            val += 1\n",
    "            rel_vec.append(1)\n",
    "        else:\n",
    "            #otherwise just append 0 for rel_vec\n",
    "            rel_vec.append(0)\n",
    "        #append the relative precision for each query document while iterating\n",
    "        # so append (number of relevant docs so far ie., val) divided by total number of documents iterated so far\n",
    "        P.append(val/(i+1))\n",
    "    count = 0\n",
    "    total = 0\n",
    "    #find the relatve precision of all the relevant documents and take sum\n",
    "    for rank in range(len(P)):\n",
    "        # for index in P list\n",
    "        # if rel_vec[i] is 1 that means it is relevant document thus increment count and add to total, else dont count\n",
    "        if rel_vec[rank] == 1:\n",
    "            count += 1\n",
    "            total += P[rank]\n",
    "    # boundary case where there is no relevent document found\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    #return the Average Precision\n",
    "    return total/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17fd8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reciprocal Rank Function\n",
    "def RR(true_list,pred_list):\n",
    "    #iterate through the ranked prediction list, break at first relevant case and return reciprocal of that rank\n",
    "    for i in range(len(pred_list)):\n",
    "        if pred_list[i] in true_list:\n",
    "            return 1/(i+1)\n",
    "        \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50769799",
   "metadata": {},
   "source": [
    "## Get all the results based on the Golden citation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52379dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dataframe for results\n",
    "results = pd.DataFrame(columns=['Document id','Prec@1','Prec@5','Prec@10','Prec@R','Recall@100','AP','RR'])\n",
    "for i in tqdm(pred_df.index):\n",
    "    #fetch the details from prediction dataframe\n",
    "    query_case = pred_df.iloc[i,0]\n",
    "#     print(query_case)\n",
    "    #r = pred_df.iloc[i,1]\n",
    "    #true_list = pred_df.iloc[i,3].copy()\n",
    "    true_list = golden_citations.get(query_case)\n",
    "    r = len(true_list)\n",
    "    #pred_list = pred_df.iloc[i,4].copy()\n",
    "    pred_list = pred_df.iloc[i,4].copy()\n",
    "    prec_at_1 = prec_at_k(true_list,pred_list,1)\n",
    "    prec_at_5 = prec_at_k(true_list,pred_list,5)\n",
    "    prec_at_10 = prec_at_k(true_list,pred_list,10)\n",
    "    prec_at_r = prec_at_k(true_list,pred_list,r)\n",
    "    \n",
    "    recall_at_100 = recall_at_k(true_list,pred_list,100,r)\n",
    "    ap = AP(true_list,pred_list)\n",
    "    rr = RR(true_list,pred_list)\n",
    "    #add the details to the result dataframe\n",
    "    results = results.append({'Document id':query_case, 'Prec@1': prec_at_1, 'Prec@5': prec_at_5 , 'Prec@10': prec_at_10, 'Prec@R': prec_at_r, 'Recall@100': recall_at_100, 'AP': ap, 'RR': rr}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results[results['Recall@100']<1]) #print number of rows whose recall score is less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd70dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"save the results in CSV file for future reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa3a39",
   "metadata": {},
   "source": [
    "Include segment csv path contains which file have:\n",
    "* Document ID\t\n",
    "* FRAGMENT_SUPPRESSED\n",
    "* REFERENCE_SUPPRESSED\n",
    "* CITATION_SUPPRESSED\n",
    "* Golden_Citations\n",
    "* Difference (#FRAGMENT_SUPPRESSED + #REFERENCE_SUPPRESSED - #Golden_Citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf838b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_csv_path = \"path to the segment CSV file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segment_csv = pd.read_csv(segment_csv_path, index_col=0)\n",
    "df_segment_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1429e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Micro Precision Function\n",
    "def micro_prec(true_list,pred_list,k):\n",
    "    #define list of top k predictions\n",
    "    cor_pred = 0\n",
    "    top_k_pred = pred_list[0:k].copy()\n",
    "    #iterate throught the top k predictions\n",
    "    for doc in top_k_pred:\n",
    "        #if document in true list, then increment count of relevant predictions\n",
    "        if doc in true_list:\n",
    "            cor_pred += 1\n",
    "    #return total_relevant_predictions_in_top_k/k\n",
    "    return cor_pred, k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the dataframe for results\n",
    "results = pd.DataFrame(columns=['Document id','Prec@1','Prec@5','Prec@10','Prec@R','Recall@100','AP','RR','Correct_pred','Retrived_cases', 'Relevant_cases'])\n",
    "correct_pred = 0\n",
    "retri_cases = 0\n",
    "relevant_cases = 0\n",
    "\n",
    "for i in tqdm(pred_df.index):\n",
    "    #fetch the details from prediction dataframe\n",
    "    query_case = pred_df.iloc[i,0]\n",
    "#     print(query_case)\n",
    "    #r = pred_df.iloc[i,1]\n",
    "    #true_list = pred_df.iloc[i,3].copy()\n",
    "    true_list = golden_citations.get(query_case)\n",
    "#     print(query_case, type(query_case))\n",
    "    nf_FS = df_segment_csv.loc[df_segment_csv[\"Document ID\"]==query_case, \"#FRAGMENT_SUPPRESSED\"].iloc[0]\n",
    "    nf_RS = df_segment_csv.loc[df_segment_csv[\"Document ID\"]==query_case, \"#REFERENCE_SUPPRESSED\"].iloc[0]\n",
    "    \n",
    "# predict based on the number of FRAGMENT_SUPPRESSED or REFERENCE_SUPPRESSED mentioned in the query documents\n",
    "\n",
    "#     if nf_FS > 0:\n",
    "#         r = nf_FS\n",
    "#     else:\n",
    "#         r = nf_RS\n",
    "#     if r > 20:\n",
    "#         r = int(r/5)\n",
    "\n",
    "    r = 5 # for constant predictions for each query case\n",
    "    \n",
    "#     r = len(true_list) #predict based on actual predictions based on given golden citation file\n",
    "    #pred_list = pred_df.iloc[i,4].copy()\n",
    "#     c_p = 0\n",
    "#     r_c = 0\n",
    "    pred_list = pred_df.iloc[i,4].copy()\n",
    "    prec_at_1 = prec_at_k(true_list,pred_list,1)\n",
    "    prec_at_5 = prec_at_k(true_list,pred_list,5)\n",
    "    prec_at_10 = prec_at_k(true_list,pred_list,10)\n",
    "    prec_at_r = prec_at_k(true_list,pred_list,r)\n",
    "    \n",
    "    c_p, r_c = micro_prec(true_list,pred_list,r)\n",
    "    correct_pred += c_p\n",
    "    retri_cases += r_c\n",
    "    relevant_cases += len(true_list)\n",
    "    \n",
    "    recall_at_100 = recall_at_k(true_list,pred_list,100,r)\n",
    "    ap = AP(true_list,pred_list)\n",
    "    rr = RR(true_list,pred_list)\n",
    "    #add the details to the result dataframe\n",
    "    results = results.append({'Document id':query_case, 'Prec@1': prec_at_1, 'Prec@5': prec_at_5 , 'Prec@10': prec_at_10, 'Prec@R': prec_at_r, 'Recall@100': recall_at_100, 'AP': ap, 'RR': rr, 'Correct_pred':c_p, 'Retrived_cases':r_c, 'Relevant_cases':len(true_list)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correct Predictions: \", correct_pred)\n",
    "print(\"Retrived Cases Predictions: \", retri_cases)\n",
    "print(\"Relevant Cases: \", relevant_cases)\n",
    "\n",
    "M_pre = correct_pred/retri_cases\n",
    "M_recall = correct_pred/relevant_cases\n",
    "M_F = 2*M_pre*M_recall/ (M_pre + M_recall)\n",
    "\n",
    "print(\"Micro Precision: \", M_pre)\n",
    "print(\"Micro Recall: \", M_recall)\n",
    "print(\"Micro F-Measure: \", M_F)\n",
    "print(correct_pred, \"\\t\", retri_cases, \"\\t\", relevant_cases, \"\\t\", M_pre, \"\\t\", M_recall, \"\\t\", M_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_df[pred_df[\"Min Cosine Sim Value in TOP R\"]<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df[\"Min Cosine Sim Value in TOP R\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(pred_df[\"Min Cosine Sim Value in TOP R\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
